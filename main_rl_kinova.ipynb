{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb2a881",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from kinova_env_best_refined_curriculum_not_forget_old_learning import  kinovaGen3Env\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO, SAC,A2C\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47523c07",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_algo():\n",
    "    # Create a directory to store the log files if it doesn't exist\n",
    "    log_dir = \"./logs\"\n",
    "    if not os.path.exists(log_dir):  # Check if the directory exists\n",
    "        os.makedirs(log_dir)  # Create the directory if it doesn't exist\n",
    "\n",
    "    # Wrap the environment with the Monitor to record the results\n",
    "    env = kinovaGen3Env()  # Instantiate the custom driving environment\n",
    "    env = Monitor(env, log_dir)  # Monitor the environment and store logs in the specified directory\n",
    "\n",
    "    # Predefine the algorithm to use (PPO, SAC, or A2C)\n",
    "    algo_name = \"SAC\"  # Set the algorithm to use (SAC, PPO, or A2C)\n",
    "    if algo_name == \"PPO\":\n",
    "        model = PPO(\"MlpPolicy\", env, verbose=1)  # Choose PPO if specified\n",
    "    elif algo_name == \"SAC\":\n",
    "        model = SAC(\"MlpPolicy\", env, verbose=1)  # Choose SAC if specified\n",
    "    elif algo_name == \"A2C\":\n",
    "        model = A2C(\"MlpPolicy\", env, verbose=1)  # Choose A2C if specified\n",
    "    else:\n",
    "        raise ValueError(\"Invalid algorithm name. Please choose 'PPO', 'SAC', or 'A2C'.\")  # Raise an error if an invalid algorithm is specified\n",
    "\n",
    "    # Set up a checkpoint callback to save models periodically\n",
    "    checkpoint_callback = CheckpointCallback(save_freq=1000, save_path=\"./models\", name_prefix=f\"ur_robot_{algo_name.lower()}\")\n",
    "\n",
    "    # Train the model for a total of 10000 timesteps and save checkpoints\n",
    "    model.learn(total_timesteps=400000, callback=checkpoint_callback)\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save(f\"kinova_robot_{algo_name.lower()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634223d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_algo(model, env, n_eval_episodes=30):\n",
    "    \"\"\"\n",
    "    Evaluate the trained agent and compute success rate + mean episode reward.\n",
    "    \"\"\"\n",
    "    successes = 0\n",
    "    episode_rewards = []\n",
    "\n",
    "    for ep in range(n_eval_episodes):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        ep_reward = 0\n",
    "        last_info = {}\n",
    "\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            ep_reward += reward\n",
    "            done = terminated or truncated\n",
    "            last_info = info\n",
    "\n",
    "        episode_rewards.append(ep_reward)\n",
    "        time.sleep(100/300)\n",
    "\n",
    "        if last_info.get(\"is_successful\", False):\n",
    "            successes += 1\n",
    "\n",
    "    success_rate = successes / n_eval_episodes\n",
    "    mean_return = np.mean(episode_rewards)\n",
    "\n",
    "    print(\"\\n================ Evaluation Result ================\")\n",
    "    print(f\"Success Rate: {success_rate * 100:.2f}%  \"\n",
    "          f\"({successes}/{n_eval_episodes})\")\n",
    "    print(f\"Mean Episode Reward: {mean_return:.2f}\")\n",
    "    print(\"==================================================\\n\")\n",
    "\n",
    "    return success_rate, mean_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfce0e2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_algo():\n",
    "    # Initialize the environment\n",
    "    env = kinovaGen3Env()  # Instantiate the custom driving environment\n",
    "    \n",
    "    # Predefine the algorithm to use (PPO, SAC, or A2C)\n",
    "    algo_name = \"SAC\"  # Set the algorithm to use (SAC, PPO, or A2C)\n",
    "    if algo_name == \"PPO\":\n",
    "        model = PPO(\"MlpPolicy\", env, verbose=1)  # Choose PPO if specified\n",
    "    elif algo_name == \"SAC\":\n",
    "        model = SAC(\"MlpPolicy\", env, verbose=1)  # Choose SAC if specified\n",
    "    elif algo_name == \"A2C\":\n",
    "        model = A2C(\"MlpPolicy\", env, verbose=1)  # Choose A2C if specified\n",
    "    else:\n",
    "        raise ValueError(\"Invalid algorithm name. Please choose 'PPO', 'SAC', or 'A2C'.\")  # Raise an error if an invalid algorithm is specified\n",
    "\n",
    "    # Load the trained model\n",
    "    model = model.load(f\"./model_best_refined_curriculum_2nd_run/ur_robot_{algo_name.lower()}_350000_steps\")\n",
    "    \n",
    "    evaluate_algo(model, env, n_eval_episodes=300)\n",
    "\n",
    "    # Reset the environment and get the initial observation\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    # Test the trained model\n",
    "    while True:\n",
    "        # Use the model to predict the next action based on the current observation\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        \n",
    "        # Sleep for 1/300 seconds to control the speed of the simulation\n",
    "        time.sleep(1/300)\n",
    "        \n",
    "        # Execute the action and get the next state\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        # If the environment reaches termination or truncation, reset it\n",
    "        if terminated or truncated:\n",
    "            #print(\"Episode finished. Waiting 1 second...\")\n",
    "            #time.sleep(1.0)\n",
    "            obs, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a44b1e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Main function to run the training or testing\n",
    "def main():\n",
    "    #train model\n",
    "    #train_algo()  # Uncomment this line to train the model\n",
    "    #test model\n",
    "    test_algo()  # Call the test function to evaluate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422be601",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function to smooth the data using a moving average\n",
    "def smooth(data, window_size=10):\n",
    "    \"\"\"Apply weighted moving average smoothing to the data\"\"\"\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0db6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the training reward data\n",
    "def plot_reward_data():\n",
    "    # Set the directory for storing Monitor log files\n",
    "    log_dir = \"./logs\"\n",
    "    files = {\n",
    "        #\"PPO\": \"monitor_ppo.csv\",\n",
    "        #\"A2C\": \"monitor_a2c.csv\",\n",
    "        \"SAC\": \"monitor.csv\"\n",
    "    }\n",
    "\n",
    "    # Create a plot for the rewards over time\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Iterate through the files for each algorithm\n",
    "    for label, file_name in files.items():\n",
    "        monitor_file = os.path.join(log_dir, file_name)\n",
    "        \n",
    "        # Read the Monitor log data (skip the first row of comments)\n",
    "        data = pd.read_csv(monitor_file, skiprows=1)\n",
    "        \n",
    "        # Calculate the cumulative timestep\n",
    "        data['timestep'] = np.cumsum(data['l'])  # Accumulate the timestep, as global timestep\n",
    "        data['timestep'] -= data['timestep'].iloc[0]  \n",
    "        \n",
    "        # Apply smoothing to the reward data\n",
    "        smoothed_rewards = smooth(data['r'])\n",
    "\n",
    "        # --- FIX: convert pandas Series â†’ numpy arrays ---\n",
    "        x = data['timestep'].to_numpy()[:len(smoothed_rewards)]\n",
    "        y = np.asarray(smoothed_rewards).reshape(-1)\n",
    "        # --------------------------------------------------\n",
    "\n",
    "        # Plot\n",
    "        plt.plot(x, y, label=label)\n",
    "    \n",
    "    # Add labels and title to the plot\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Training Reward\")\n",
    "    \n",
    "    # Add legend to the plot\n",
    "    plt.legend()\n",
    "    plt.grid()  # Display grid lines\n",
    "    plt.show()  # Show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107adb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main entry point for the script\n",
    "if __name__ == '__main__':\n",
    "    main()  # Run the main function to either train or test the model\n",
    "    #plot_reward_data()  # Plot the training reward data"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
